{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Setup Instructions\n",
    "\n",
    "## 1. Create a Virtual Environment\n",
    "```bash\n",
    "python -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On Windows:\n",
    "venv\\Scripts\\activate\n",
    "# On Unix or MacOS:\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "## 2. Create Environment File\n",
    "1. Create a new file named `.env` in the root directory of the project\n",
    "2. Add the following line to the file:\n",
    "   ```\n",
    "   GROQ = \"your_api_key_here\"\n",
    "   ```\n",
    "   > **Note**: You can get a free API key by creating an account at [Groq's website](https://www.groq.com/). This is a free-of-charge service as requested for using free models.\n",
    "\n",
    "## 3. Install Dependencies\n",
    "You have two options:\n",
    "- Install from requirements file:\n",
    "  ```bash\n",
    "  pip install -r requirements.txt\n",
    "  ```\n",
    "- Or run the first cell in the notebook which contains the package installations\n",
    "\n",
    "After completing these steps, you should be able to run the notebook successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-text-splitters\n",
    "%pip install -qU langchain-community langchain-text-splitters \"unstructured[md]\" nltk\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU sentence-transformers\n",
    "%pip install -qU faiss-cpu\n",
    "%pip install rank_bm25\n",
    "%pip install -qU anthropic\n",
    "%pip install langgraph\n",
    "%pip install langchain-groq\n",
    "%pip install pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I'd like to share my ideas and thought process behind the development of this project:\n",
    "\n",
    "Through my TDK and [thesis](https://diplomaterv.vik.bme.hu/hu/Theses/Megbeszelesek-szemelyre-szabott-osszegzese-es) work, I gained experience with multi-agent systems focused on creating realistic meeting transcripts. This involved converting unstructured information into detailed summaries, and further transforming those into structured formats like tasks and personal summaries. \n",
    "\n",
    "To leverage this previous work, I decided to use my existing meeting transcripts and detailed summaries as the data source for this project. I envisioned a system where these meeting summaries would be processed using a markdown-aware splitter for chunking, then embedded to enable efficient information retrieval. \n",
    "\n",
    "Given my thesis experience with multi-step and multi-agent workflows, I wanted to incorporate this approach into the current system too.\n",
    "\n",
    "![Architecture](./figures/pwc.png)\n",
    "\n",
    "\n",
    "\n",
    "The system consists of 2 main parts:\n",
    "\n",
    "User Interaction - This component handles user questions through multiple agents:\n",
    "\n",
    "- First, an agent analyzes the question and determines the steps needed for answering, following a \"planning\" approach\n",
    "- Second, an agent uses the plan, task, and basic contextual information to identify what additional information is needed\n",
    "- Third, this information is used to query the vector database for relevant content\n",
    "- Finally, an agent combines all this information to generate the answer\n",
    "\n",
    "\n",
    "RAG System - This handles information retrieval using both BM25 and FAISS:\n",
    "\n",
    "- BM25 excels when words align directly\n",
    "- FAISS captures semantic meaning\n",
    "I have previous experience with both approaches, making them natural choices for this implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded note_9.md\n",
      "Successfully loaded note_14.md\n",
      "Successfully loaded note_10.md\n",
      "Successfully loaded note_11.md\n",
      "Successfully loaded note_8.md\n",
      "Successfully loaded note_15.md\n",
      "Successfully loaded note_3.md\n",
      "Successfully loaded note_7.md\n",
      "Successfully loaded note_6.md\n",
      "Successfully loaded note_2.md\n",
      "Successfully loaded note_5.md\n",
      "Successfully loaded note_18.md\n",
      "Successfully loaded note_1.md\n",
      "Successfully loaded note_4.md\n",
      "Successfully loaded note_12.md\n",
      "Successfully loaded note_16.md\n",
      "Successfully loaded note_17.md\n",
      "Successfully loaded note_13.md\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def load_markdown_files(directory_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.md'):\n",
    "            try:\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                # Simple raw text loading\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    # Create a Document with the raw content and metadata\n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\"source\": filename}\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                print(f\"Successfully loaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {str(e)}\")\n",
    "    return documents\n",
    "\n",
    "# Load all documents\n",
    "markdown_directory = \"data/note/\"  \n",
    "documents = load_markdown_files(markdown_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 11 markdown documents and 7 plaintext documents\n",
      "Total chunks created: 268\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Some of the used data does not contain # as for markdown it is needed so i added a basic recursive chunking method which splits on new lines\n",
    "# This is a cool site to visualize the chunks: i used this and went with 1200: https://chunkviz.up.railway.app/\n",
    "\n",
    "def smart_split_documents(documents):\n",
    "    markdown_docs = []\n",
    "    plaintext_docs = []\n",
    "    \n",
    "    # Categorize documents\n",
    "    for doc in documents:\n",
    "        if any(line.strip().startswith('#') for line in doc.page_content.split('\\n')):\n",
    "            markdown_docs.append(doc)\n",
    "        else:\n",
    "            plaintext_docs.append(doc)\n",
    "    \n",
    "    all_splits = []\n",
    "    \n",
    "    # Process markdown documents\n",
    "    if markdown_docs:\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "            (\"####\", \"Header 4\"),\n",
    "        ]\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "        \n",
    "        for doc in markdown_docs:\n",
    "            try:\n",
    "                splits = markdown_splitter.split_text(doc.page_content)\n",
    "                all_splits.extend(splits)\n",
    "            except Exception as e:\n",
    "                print(f\"Error splitting markdown document: {str(e)}\")\n",
    "    \n",
    "    # Process plaintext documents\n",
    "    if plaintext_docs:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1200,\n",
    "            chunk_overlap=0,\n",
    "            length_function=len,\n",
    "        )\n",
    "        plaintext_splits = text_splitter.split_documents(plaintext_docs)\n",
    "        all_splits.extend(plaintext_splits)\n",
    "    \n",
    "    print(f\"Split {len(markdown_docs)} markdown documents and {len(plaintext_docs)} plaintext documents\")\n",
    "    print(f\"Total chunks created: {len(all_splits)}\")\n",
    "    \n",
    "    return all_splits\n",
    "\n",
    "# Use the smart splitter\n",
    "chunks = smart_split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "store = LocalFileStore(\"./shared_cache/\")\n",
    "# HuggingFace embeddings - I found this - to be honest i dont know too much about this model I used OpenAI embeddings before\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "    store,\n",
    "    namespace=\"hf-miniLM-l6\",\n",
    "    \n",
    ")\n",
    "\n",
    "# Create FAISS vectorstore\n",
    "faiss_db = FAISS.from_documents(chunks, embedder)\n",
    "faiss_retriever = faiss_db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Create BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Results:\n",
      "\n",
      "--- Result 1 ---\n",
      "Metadata: {}\n",
      "Content: VII. Definition of Done  \n",
      "The team established the following Definition of Done criteria: 1. Code review completion 2. Unit tests with at least 80% coverage 3. Integration tests for critical paths 4. Successful deployment to staging 5. No critical or high-priority bugs 6. API documentation (added by Michael Kim) 7. Accessibility testing to WCAG 2.1 AA standards (added by Liam Foster) 8. Security testing (added by Alex Rodriguez)  \n",
      "VIII. Sprint Backlog Creation  \n",
      "The team prioritized the following high-priority items: 1. User authentication 2. Basic database structure for user profiles 3. Basic dashboard structure and activity input forms  \n",
      "Detailed breakdown for user authentication: - Security requirements: secure password hashing, rate limiting for login attempts, JWT token management with proper expiration, and secure session handling. - Backend implementation (Michael Kim): 3 days, including testing - Frontend implementation (Emily Watson): 4 days, including error handling and UI components - UI design (Liam Foster): 1 day for authentication flow designs  \n",
      "IX. User Profile Functionality  \n",
      "Michael Kim estimated 5 days for backend implementation of the profile system.\n",
      "\n",
      "--- Result 2 ---\n",
      "Metadata: {}\n",
      "Content: Action Items: 1. Alex Rodriguez: Research nutrition APIs and prepare comparison (Due: Next day) 2. Alex Rodriguez: Draft security protocols for CI/CD pipeline (Due: EOD) 3. Emily Watson & Liam Foster: Collaborate on authentication flow design 4. Michael Kim: Start database schema development and share documentation (Due: EOD) 5. Olivia Martinez: Begin CI/CD pipeline setup (Estimated: 3 days) 6. All team members: Send SSH keys to Olivia Martinez (Due: EOD) 7. Liam Foster: Update design system documentation with meeting decisions 8. Sarah Chen: Send out meeting notes and action items (Due: Within the hour) 9. Sarah Chen: Schedule technical sync for 10 AM next day 10. Sarah Chen: Schedule design review for Thursday morning\n",
      "\n",
      "BM25 Results:\n",
      "\n",
      "--- Result 1 ---\n",
      "Metadata: {'Header 1': 'HealthTrack Pro Daily Scrum Meeting - June 17, 2024', 'Header 2': 'V. Non-Sprint Tasks and Reminders (Prioritized)'}\n",
      "Content: 1. High Priority: Alex to follow up on office security software license renewal approval\n",
      "2. Medium Priority: Sarah to track arrival of new printer cartridges\n",
      "3. Low Priority: Michael to continue office floor plan review\n",
      "4. Low Priority: Liam to install new office door handle upon arrival\n",
      "\n",
      "--- Result 2 ---\n",
      "Metadata: {'Header 1': 'HealthTrack Pro: Sprint Planning Meeting (June 10, 2024)', 'Header 2': 'X. Non-Project Related Tasks'}\n",
      "Content: The team confirmed that the non-project related tasks could be handled without significantly impacting sprint capacity:  \n",
      "1. Liam: Will update backup drives after hours\n",
      "2. Alex: Will renew password manager subscription during lunch break\n",
      "3. Sarah: Will handle kitchen sink repair scheduling\n",
      "4. Michael: Will purchase monitor stands between tasks\n",
      "5. Olivia: Will spread digital asset library organization over several days\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "query = \"door handle repair\"  \n",
    "\n",
    "# Get results from both retrievers\n",
    "faiss_results = faiss_retriever.get_relevant_documents(query)\n",
    "bm25_results = bm25_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print results\n",
    "print(\"FAISS Results:\")\n",
    "for i, doc in enumerate(faiss_results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "\n",
    "print(\"\\nBM25 Results:\")\n",
    "for i, doc in enumerate(bm25_results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be honest I'm not too satisfied with the accuracy. \n",
    "I did a small testing with it and I found that BM25 works a bit better for the kind of information i searched for. Mainly administrative tasks like pest control or door handle repair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tipical RAG pipeline look like these embedding and searching but what i found in my recent work as that the models need to a carefully collected contextual data. Since it is very easy to misslead the models. I can add these as cotnext to my prompts but Im 100 percent sure this wont make perfect results. Also right now the biggest challenge is getting these LLM based systems robust. With inconsistent context collection this just makes things worse. \n",
    "\n",
    "One important step i believe is to try to find what informations are needed to be able to answer the question. You will see in the next agentic pipeline I add several contextual discription to the model as context. This helps a lot when talking about robustness. These will go into the XML tags into the prompts. Also there are two steps one for planning and one for finding out what information could be needed to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANNER_PROMPT= \"\"\"\n",
    "You are a planning agent in a multi-agent QA pipeline. Your role is to analyze user questions and create structured plans for answering them. You have access to basic context information and work alongside a context collection agent.\n",
    "\n",
    "Here is the basic context information available to you, these are same that the final anwswer agent will get:\n",
    "These informations are about the company, the running project and its state as well as information about the employees.\n",
    "<basic_context>\n",
    "    <company_data>\n",
    "    {COMPANY_DATA}\n",
    "    </company_data>\n",
    "\n",
    "    <project_general>\n",
    "    {PROJECT_GENERAL}\n",
    "    </project_general>\n",
    "\n",
    "    <employee_profiles>\n",
    "    {EMPLOYEE_PROFILES}\n",
    "    </employee_profiles>\n",
    "\n",
    "    <project_state>\n",
    "    {PROJECT_STATE}\n",
    "    </project_state>\n",
    "</basic_context>\n",
    "\n",
    "The user has asked the following question:\n",
    "<user_question>\n",
    "{USER_QUESTION}\n",
    "</user_question>\n",
    "\n",
    "Your task is to create a clear, step-by-step plan for answering this question. Follow these guidelines:\n",
    "\n",
    "1. Carefully analyze the question to identify its main components and any sub-questions it might contain.\n",
    "\n",
    "2. Consider what aspects of the question need to be addressed to provide a complete answer.\n",
    "\n",
    "3. Break down complex questions into manageable parts if necessary.\n",
    "\n",
    "4. Think about what types of information might be needed to answer each part of the question.\n",
    "\n",
    "5. Consider how to structure the final answer in a logical and coherent manner.\n",
    "\n",
    "6. Keep in mind the basic context information provided and how it might be relevant to answering the question.\n",
    "\n",
    "7. If there are potential limitations or areas where more information might be needed, include steps to address these.\n",
    "\n",
    "Create your plan as a numbered list of steps, with brief explanations for each step. This plan will guide the answer agent in providing a complete and accurate response.\n",
    "\n",
    "Begin your response with \"Here is the plan to answer the user's question:\" and then provide your numbered list of steps. Each step should be concise but clear, explaining what needs to be done and why.\n",
    "\n",
    "Remember to consider only the information available in the basic context and what could reasonably be inferred from the question itself. Do not assume access to information or capabilities not mentioned in the context.\n",
    "\n",
    "Provide your plan inside <plan> tags.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_COLLECTOR_PROMPT =  \"\"\"\n",
    "You are a context collection agent in a multi-agent QA pipeline. Your role is to identify what specific information needs to be retrieved from the knowledge base to answer the user's question effectively.\n",
    "\n",
    "These informations are about the company, the running project and its state as well as information about the employees.\n",
    "These are the information that the final agent will get too. If the information that is need to be able to answer the user question is in this context than you dont need to contaion that in the query, since the next agent will get these as you did.\n",
    "<basic_context>\n",
    "    <company_data>\n",
    "    {COMPANY_DATA}\n",
    "    </company_data>\n",
    "\n",
    "    <project_general>\n",
    "    {PROJECT_GENERAL}\n",
    "    </project_general>\n",
    "\n",
    "    <employee_profiles>\n",
    "    {EMPLOYEE_PROFILES}\n",
    "    </employee_profiles>\n",
    "\n",
    "    <project_state>\n",
    "    {PROJECT_STATE}\n",
    "    </project_state>\n",
    "</basic_context>\n",
    "\n",
    "\n",
    "Here is the user's question:\n",
    "<user_question>\n",
    "{USER_QUESTION}\n",
    "</user_question>\n",
    "\n",
    "Here is the plan that has been developed to answer this question:\n",
    "<plan>\n",
    "{PLAN}\n",
    "</plan>\n",
    "\n",
    "\n",
    "Your task is to analyze the user's question and the provided plan to determine what specific information is needed to answer the question completely. Consider the following:\n",
    "\n",
    "1. What key facts or data points are necessary to provide a comprehensive answer?\n",
    "2. Are there any domain-specific concepts or terminology that need to be explained?\n",
    "3. What background information might be required to fully understand the context of the question?\n",
    "4. Are there any specific details mentioned in the plan that require additional information?\n",
    "\n",
    "Based on your analysis, create a list of all the required information pieces. Each piece of information should be concise and specific.\n",
    "\n",
    "Present your response as a comma-separated list of required information pieces. Do not include explanations or justifications for your choices. Simply list the information needed.\n",
    "\n",
    "For example, your output might look like this:\n",
    "<output>meeting summary from last sprint, project timeline details, team member responsibilities, budget allocation for Q3, client feedback on prototype</output>\n",
    "\n",
    "Remember to focus on information that is directly relevant to answering the user's question and executing the provided plan. Avoid listing unnecessary or tangential information.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PROMPT = \"\"\"\n",
    "You are an AI agent tasked with generating a correct and comprehensive answer to a user's question. Follow these instructions carefully:\n",
    "\n",
    "Here is the question you need to answer:\n",
    "<user_question>\n",
    "{USER_QUESTION}\n",
    "</user_question>\n",
    "\n",
    "1. First, understand your role and the system you're working in:\n",
    "These informations are about the company, the running project and its state as well as information about the employees.\n",
    "<basic_context>\n",
    "    <company_data>\n",
    "    {COMPANY_DATA}\n",
    "    </company_data>\n",
    "\n",
    "    <project_general>\n",
    "    {PROJECT_GENERAL}\n",
    "    </project_general>\n",
    "\n",
    "    <employee_profiles>\n",
    "    {EMPLOYEE_PROFILES}\n",
    "    </employee_profiles>\n",
    "\n",
    "    <project_state>\n",
    "    {PROJECT_STATE}\n",
    "    </project_state>\n",
    "</basic_context>\n",
    "\n",
    "2. Review the following context, which includes the retrieved context from the knowledge base, and that might be needed to asnwer user question:\n",
    "<context>\n",
    "{CONTEXT}\n",
    "</context>\n",
    "\n",
    "3. Your task is to create a comprehensive answer. Follow this plan:\n",
    "{PLAN}\n",
    "\n",
    "4. Before providing your final answer, use a scratchpad to organize your thoughts and plan your response. Write your planning process inside <scratchpad> tags.\n",
    "\n",
    "5. After planning, provide your final answer inside <answer> tags. Ensure your answer is clear, well-structured, and directly addresses the user's question while following the provided plan.\n",
    "\n",
    "As a reminder you need to answer to the following user question:\n",
    "<user_question>\n",
    "{USER_QUESTION}\n",
    "</user_question> \n",
    "\n",
    "Remember to incorporate relevant information from the context, maintain clarity and coherence, and stay focused on the specific question asked by the user.\n",
    "If you cant answer since that information is not in your context window say that \"I dont have clear data on that sorry\".\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict, Any, List\n",
    "class QAState(TypedDict):\n",
    "    #basic context about the company and project\n",
    "    company_data: str\t\n",
    "    project_general: str\t\n",
    "    employee_profiles: str\t\n",
    "    project_state : str\n",
    "\n",
    "    #user question\n",
    "    question : str\n",
    "    #plan to answer\n",
    "    plan : str\n",
    "    #query from collect context agent\n",
    "    query: str\n",
    "    #retrieved context from vector db\n",
    "    retrieved_context: str\n",
    "    #final answer\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_markdown_to_str(file_path):\n",
    "\twith open(file_path, 'r', encoding='utf-8') as md_file:\n",
    "\t\tmarkdown_content = md_file.read()\n",
    "\treturn markdown_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "\n",
    "class QAPipeline:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.state = QAState()\n",
    "        graph = StateGraph(QAState)\n",
    "\n",
    "\n",
    "        graph.add_node(\"plan_to_answer\", self.plan_to_answer)\n",
    "        graph.add_node(\"collect_context\", self.collect_context)\n",
    "        graph.add_node(\"query_context\", self.query_context)\n",
    "        graph.add_node(\"generate_answer\", self.generate_answer)\n",
    "\n",
    "        graph.add_edge(\"plan_to_answer\",\"collect_context\")\n",
    "        graph.add_edge(\"collect_context\",\"query_context\")        \n",
    "        graph.add_edge(\"query_context\",\"generate_answer\")\n",
    "        graph.add_edge(\"generate_answer\",END)\n",
    "        \n",
    "        graph.set_entry_point(\"plan_to_answer\")\n",
    "        self.graph = graph.compile()\n",
    "\n",
    "        \n",
    "    def initialize_state(self, user_question : str):\n",
    "        print(\"--- INITIALIZE STATE NODE ---\")\n",
    "        self.state = QAState(\n",
    "            company_data= load_markdown_to_str(os.path.join(\"data\",\"company-general.md\")),\n",
    "            project_general= load_markdown_to_str(os.path.join(\"data\",\"project-general.md\")),\n",
    "            employee_profiles= load_markdown_to_str(os.path.join(\"data\",\"employee-profiles.md\")),\n",
    "            project_state= load_markdown_to_str(os.path.join(\"data\",\"project-state.md\")),\n",
    "\n",
    "            question = user_question,\n",
    "\n",
    "            plan = \"\",\n",
    "            query = \"\",\n",
    "            retrieved_context = \"\",\n",
    "            answer = \"\"\n",
    "\n",
    "        )\n",
    "        return self.state\n",
    "        \n",
    "    def plan_to_answer(self, state: QAState) -> QAState:\n",
    "        print(\"--- PLAN TO ANSWER NODE ---\")\n",
    "        formatted_prompt = PLANNER_PROMPT.format(\n",
    "            COMPANY_DATA = self.state.get(\"company_data\"),\n",
    "            PROJECT_GENERAL = self.state.get(\"project_general\"),\n",
    "            EMPLOYEE_PROFILES = self.state.get(\"employee_profiles\"),\n",
    "            PROJECT_STATE = self.state.get(\"project_state\"),\n",
    "            USER_QUESTION = self.state.get(\"question\"),\n",
    "        )\n",
    "        messages = [\n",
    "            HumanMessage(content=formatted_prompt)\n",
    "        ]\n",
    "        response = self.model.invoke(messages)\n",
    "        \n",
    "        self.state[\"plan\"] = response.content\n",
    "        print(self.state.get(\"plan\"))\n",
    "        return self.state\n",
    "    \n",
    "    def collect_context(self, state : QAState) -> QAState:\n",
    "        print(\"--- collect context node ---\")\n",
    "        formatted_prompt = CONTEXT_COLLECTOR_PROMPT.format(\n",
    "            COMPANY_DATA = self.state.get(\"company_data\"),\n",
    "            PROJECT_GENERAL = self.state.get(\"project_general\"),\n",
    "            EMPLOYEE_PROFILES = self.state.get(\"employee_profiles\"),\n",
    "            PROJECT_STATE = self.state.get(\"project_state\"),\n",
    "            USER_QUESTION = self.state.get(\"question\"),\n",
    "            PLAN = self.state.get(\"plan\")\n",
    "        )\n",
    "        messages = [\n",
    "            HumanMessage(content=formatted_prompt)\n",
    "        ]\n",
    "        response = self.model.invoke(messages)\n",
    "        self.state[\"query\"] = response.content\n",
    "        print(self.state.get(\"query\"))\n",
    "        return self.state\n",
    "\n",
    "    \n",
    "    def query_context(self, state: QAState):\n",
    "        print(\"--- QUERY CONTEXT ---\")\n",
    "        \n",
    "        # Split the query into a list of strings and clean them\n",
    "        queries = [q.strip() for q in self.state.get(\"query\").split(\",\")]\n",
    "        print(f\"Processing queries: {queries}\")\n",
    "        \n",
    "        all_retrieved_contexts = []\n",
    "        \n",
    "        # Process each query separately\n",
    "        for query in queries:\n",
    "            print(f\"\\nRetrieving context for: '{query}'\")\n",
    "            \n",
    "            # Get results from both retrievers\n",
    "            faiss_results = faiss_retriever.get_relevant_documents(query)\n",
    "            bm25_results = bm25_retriever.get_relevant_documents(query)\n",
    "            \n",
    "            # Format FAISS results\n",
    "            faiss_context = \"\\n=== FAISS Results ===\\n\"\n",
    "            for doc in faiss_results:\n",
    "                faiss_context += f\"\\nSource: {doc.metadata.get('source', 'Unknown')}\\n\"\n",
    "                faiss_context += f\"{doc.page_content}\\n\"\n",
    "                faiss_context += \"-\" * 50 + \"\\n\"\n",
    "            \n",
    "            # Format BM25 results\n",
    "            bm25_context = \"\\n=== BM25 Results ===\\n\"\n",
    "            for doc in bm25_results:\n",
    "                bm25_context += f\"\\nSource: {doc.metadata.get('source', 'Unknown')}\\n\"\n",
    "                bm25_context += f\"{doc.page_content}\\n\"\n",
    "                bm25_context += \"-\" * 50 + \"\\n\"\n",
    "            \n",
    "            # Combine results for this query\n",
    "            query_context = f\"\\n=== Query: '{query}' ===\\n\"\n",
    "            query_context += faiss_context + bm25_context\n",
    "            \n",
    "            all_retrieved_contexts.append(query_context)\n",
    "        \n",
    "        # Combine all retrieved contexts\n",
    "        self.state[\"retrieved_context\"] = \"\\n\".join(all_retrieved_contexts)\n",
    "        \n",
    "        print(f\"\\nTotal retrieved context length: {len(self.state['retrieved_context'])} characters\")\n",
    "        return self.state\n",
    "    \n",
    "    def generate_answer(self, state : QAState) -> QAState:\n",
    "        print(\"--- generating answer ---\")\n",
    "        formatted_prompt = ANSWER_PROMPT.format(\n",
    "            USER_QUESTION = self.state.get(\"question\"),\n",
    "            COMPANY_DATA = self.state.get(\"company_data\"),\n",
    "            PROJECT_GENERAL = self.state.get(\"project_general\"),\n",
    "            EMPLOYEE_PROFILES = self.state.get(\"employee_profiles\"),\n",
    "            PROJECT_STATE = self.state.get(\"project_state\"),\n",
    "            CONTEXT = self.state.get(\"retrieved_context\"),\n",
    "            PLAN = self.state.get(\"plan\")\n",
    "\n",
    "        )\n",
    "        messages = [\n",
    "            HumanMessage(content=formatted_prompt)\n",
    "        ]\n",
    "        response = self.model.invoke(messages)\n",
    "        self.state[\"answer\"] = response.content\n",
    "        print(self.state.get(\"answer\"))\n",
    "        return self.state\n",
    "    \n",
    "    def run(self, user_question: str):\n",
    "        print(\"--- RUN ---\")\n",
    "        self.initialize_state(user_question)\n",
    "        self.graph.invoke(self.state)\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ\")\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",  # Replace with the exact model name if different\n",
    "    temperature=0.1,               # Adjust temperature as needed\n",
    "    max_tokens=2048,               # Set max tokens based on your requirements\n",
    "    timeout=30,                    # Set timeout as needed\n",
    "    max_retries=3,                  # Set max retries for API calls\n",
    "    api_key = groq_api_key\n",
    ")\n",
    "agent = QAPipeline(model=model)\n",
    "#Image(agent.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUN ---\n",
      "--- INITIALIZE STATE NODE ---\n",
      "--- PLAN TO ANSWER NODE ---\n",
      "Here is the plan to answer the user's question:\n",
      "<plan>\n",
      "1. Identify the key components of the question, which are \"responsible\" and \"backend development\", to understand that the question is asking about the person or role in charge of backend development.\n",
      "2. Review the basic context information provided, specifically the <employee_profiles> section, to find details about the roles and responsibilities of the employees at TechNova Solutions.\n",
      "3. Look for the employee profile that mentions \"Backend Developer\" or responsibilities related to backend development, such as server-side logic, database management, or API development.\n",
      "4. Extract the name and relevant details of the employee responsible for backend development to provide a direct answer to the question.\n",
      "5. Consider if there are any additional details from the context that might be relevant to the answer, such as the technology stack used for backend development.\n",
      "6. Structure the final answer to clearly state the name of the employee responsible for backend development and optionally mention their relevant skills or responsibilities.\n",
      "7. Ensure the answer is concise and directly addresses the user's question, avoiding unnecessary information.\n",
      "</plan>\n",
      "--- collect context node ---\n",
      "employee name responsible for backend development, role description of backend developer, relevant skills for backend development, technology stack used for backend development, team structure and responsibilities.\n",
      "--- QUERY CONTEXT ---\n",
      "Processing queries: ['employee name responsible for backend development', 'role description of backend developer', 'relevant skills for backend development', 'technology stack used for backend development', 'team structure and responsibilities.']\n",
      "\n",
      "Retrieving context for: 'employee name responsible for backend development'\n",
      "\n",
      "Retrieving context for: 'role description of backend developer'\n",
      "\n",
      "Retrieving context for: 'relevant skills for backend development'\n",
      "\n",
      "Retrieving context for: 'technology stack used for backend development'\n",
      "\n",
      "Retrieving context for: 'team structure and responsibilities.'\n",
      "\n",
      "Total retrieved context length: 8526 characters\n",
      "--- generating answer ---\n",
      "<scratchpad>\n",
      "To answer the user's question, \"How is responsible for backend development?\", we need to identify the employee at TechNova Solutions who is responsible for backend development. \n",
      "\n",
      "Reviewing the <employee_profiles> section, we find that Michael Kim is listed as the \"Backend Developer\". His responsibilities include developing and maintaining server-side logic, designing and implementing database schemas, creating and documenting APIs, and implementing data processing and analysis features.\n",
      "\n",
      "Additionally, we can note that Alex Rodriguez, as a Senior Full-Stack Developer, also has expertise in backend technologies, including Node.js and Express.js, and is involved in technical decisions and architecture design. However, the primary responsibility for backend development is assigned to Michael Kim.\n",
      "\n",
      "Given this information, we can structure our answer to clearly state that Michael Kim is the employee responsible for backend development, highlighting his relevant skills and responsibilities if necessary.\n",
      "</scratchpad>\n",
      "\n",
      "<answer>\n",
      "Michael Kim is responsible for backend development at TechNova Solutions. As the Backend Developer, his responsibilities include developing and maintaining server-side logic, database management, and API development, utilizing technologies such as Node.js, Express.js, and PostgreSQL.\n",
      "</answer>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'company_data': '# TechNova Solutions\\n\\n## Company Overview\\nTechNova Solutions is a small, dynamic IT company specializing in web application development. With a team of 6 skilled professionals, they focus on creating innovative, user-friendly web solutions for small to medium-sized businesses.\\n\\n## Current Project: HealthTrack Pro\\nTechNova is developing HealthTrack Pro, a comprehensive web application for personal health management. This application allows users to track their daily activities, nutrition, and health metrics, and provides insights and recommendations for a healthier lifestyle.\\n\\n## Team Structure\\n1. ** Sarah Chen - Project Manager / Scrum Master**\\n   - Oversees project progress, manages timelines, and facilitates communication\\n   - Has a background in both frontend and backend development\\n\\n2. ** Alex Rodriguez - Senior Full-Stack Developer**\\n   - Leads technical decisions and architecture design\\n   - Proficient in both frontend and backend technologies\\n\\n3. ** Emily Watson - Frontend Developer**\\n   - Specializes in creating responsive and intuitive user interfaces\\n   - Skilled in modern frontend frameworks and UX design\\n\\n4. ** Michael Kim - Backend Developer**\\n   - Focuses on server-side logic, database management, and API development\\n   - Experienced in cloud technologies and scalable architectures\\n\\n5. ** Olivia Martinez - QA Engineer / DevOps Specialist**\\n   - Ensures product quality through testing and continuous integration\\n   - Manages deployment processes and infrastructure\\n\\n6. ** Liam Foster - UI/UX Designer**\\n   - Creates user-centric designs and prototypes\\n   - Collaborates closely with frontend and backend teams for seamless implementation\\n\\n## Technology Stack\\n- Frontend: React.js, TypeScript, Tailwind CSS\\n- Backend: Node.js, Express.js, PostgreSQL\\n- DevOps: Docker, AWS, Jenkins\\n- Design: Figma, Adobe Creative Suite\\n\\n',\n",
       " 'project_general': '# Current Project: HealthTrack Pro\\nTechNova is developing HealthTrack Pro, a comprehensive web application for personal health management. This application allows users to track their daily activities, nutrition, and health metrics, and provides insights and recommendations for a healthier lifestyle.\\n\\nThe project started in 2024 june 10.\\n\\n## Project Components\\n1. User Authentication and Profile Management\\n2. Activity Tracking (steps, exercise, sleep)\\n3. Nutrition Logging and Analysis\\n4. Health Metrics Dashboard (weight, heart rate, blood pressure)\\n5. Goal Setting and Progress Tracking\\n6. Recommendation Engine for personalized health advice\\n7. Social Features (friend connections, challenges)\\n8. Integration with popular fitness devices and apps\\n\\n## Technology Stack\\n- Frontend: React.js, TypeScript, Tailwind CSS\\n- Backend: Node.js, Express.js, PostgreSQL\\n- DevOps: Docker, AWS, Jenkins\\n- Design: Figma, Adobe Creative Suite\\n\\n',\n",
       " 'employee_profiles': '# TechNova Solutions: Detailed Employee Profiles\\n\\n## 1. Sarah Chen - Project Manager / Scrum Master\\n- Responsibilities:\\n  - Oversee project progress and timelines\\n  - Facilitate communication between team members and stakeholders\\n  - Conduct sprint planning, daily stand-ups, and retrospectives\\n  - Manage project risks and resources\\n- Skills:\\n  - Certified Scrum Master\\n  - Proficient in Agile methodologies\\n  - Experienced with project management tools (Jira, Trello)\\n  - Basic understanding of both frontend (React) and backend (Node.js) development\\n  - Excellent communication and leadership skills\\n\\n## 2. Alex Rodriguez - Senior Full-Stack Developer\\n- Responsibilities:\\n  - Lead technical decisions and architecture design\\n  - Mentor junior developers\\n  - Implement complex features across the stack\\n  - Code review and quality assurance\\n- Skills:\\n  - Expert in React.js, Node.js, and Express.js\\n  - Proficient in database design and management (PostgreSQL)\\n  - Experienced with cloud services (AWS)\\n  - Strong problem-solving and system design skills\\n  - Knowledgeable in security best practices and performance optimization\\n\\n## 3. Emily Watson - Frontend Developer\\n- Responsibilities:\\n  - Implement responsive and intuitive user interfaces\\n  - Collaborate with UI/UX designer to bring designs to life\\n  - Optimize frontend performance\\n  - Ensure cross-browser compatibility\\n- Skills:\\n  - Expert in React.js and TypeScript\\n  - Proficient in HTML5, CSS3, and JavaScript\\n  - Experienced with state management (Redux, MobX)\\n  - Knowledgeable in frontend testing frameworks (Jest, React Testing Library)\\n  - Familiar with UI component libraries and CSS-in-JS solutions\\n\\n## 4. Michael Kim - Backend Developer\\n- Responsibilities:\\n  - Develop and maintain server-side logic\\n  - Design and implement database schemas\\n  - Create and document APIs\\n  - Implement data processing and analysis features\\n- Skills:\\n  - Expert in Node.js and Express.js\\n  - Proficient in database management (PostgreSQL, MongoDB)\\n  - Experienced with RESTful API design and GraphQL\\n  - Knowledgeable in data structures and algorithms\\n  - Familiar with microservices architecture\\n\\n## 5. Olivia Martinez - QA Engineer / DevOps Specialist\\n- Responsibilities:\\n  - Develop and execute test plans and test cases\\n  - Set up and maintain CI/CD pipelines\\n  - Manage deployment processes and infrastructure\\n  - Monitor system performance and security\\n- Skills:\\n  - Proficient in manual and automated testing (Selenium, Cypress)\\n  - Experienced with containerization (Docker) and orchestration (Kubernetes)\\n  - Knowledgeable in cloud platforms (AWS, Google Cloud)\\n  - Familiar with monitoring tools (ELK stack, Prometheus)\\n  - Basic scripting skills (Python, Bash)\\n\\n## 6. Liam Foster - UI/UX Designer\\n- Responsibilities:\\n  - Create user-centric designs and prototypes\\n  - Conduct user research and usability testing\\n  - Develop and maintain the design system\\n  - Collaborate with developers to ensure design integrity\\n- Skills:\\n  - Expert in design tools (Figma, Adobe Creative Suite)\\n  - Proficient in interaction design and prototyping\\n  - Experienced with user research methodologies\\n  - Knowledgeable in accessibility standards\\n  - Basic understanding of HTML and CSS\\n',\n",
       " 'project_state': \"Here's the updated project state based on the meeting memo:\\n\\n<updated_project_state>\\n\\n# HealthTrack Pro Project State\\n\\n## Sprint 1 (Closed)\\n[Content remains unchanged]\\n\\n## Sprint 2 (Current)\\nStart Date: 2024-07-04\\nEnd Date: 2024-07-17\\nDemo Date: 2024-07-18\\n\\n### Sprint Goals\\n[Content remains unchanged]\\n\\n### Key Decisions\\n- Implement intermediate confirmation step specifically for Safari users in WebAuthn flow\\n- Extend token expiration grace period to 30 seconds for Safari users\\n- Implement a queue system with Redis-based queue and exponential backoff for failed authentication attempts\\n- Add visual indicators and ARIA announcements for the authentication process\\n- Implement a hybrid partitioning strategy with modified hash function to address distribution issues\\n- Postpone major schema changes until Michael's return from the conference\\n\\n### User Stories and Tasks\\n\\n1. Health Metrics Dashboard Implementation (34 story points)\\n   - Implement database partitioning changes (13 points, Michael Kim)\\n     - Status: In Progress\\n     - Update: Documentation 80% complete, 40% reduction in query response times achieved\\n     - Challenge: Potential bottleneck identified for user profiles beyond 100,000 users\\n     - Next steps: Review modified hash function with Alex, set up Grafana dashboards for monitoring partition metrics\\n   - Develop frontend components for real-time updates (Emily Watson, Liam Foster)\\n     - Status: In Progress\\n     - Update: Implemented visual indicators for authentication process and ARIA announcements for screen readers\\n     - Challenge: Inconsistent behavior across different screen readers, especially in Safari\\n   - Implement service layer improvements for nutrition API (8 points, Alex Rodriguez)\\n     - Status: In Progress\\n     - Update: Implemented Redis-based queue with exponential backoff for failed authentication attempts\\n\\n2. Nutrition Logging Feature Completion\\n   - Implement offline support and data synchronization (Emily Watson, Michael Kim)\\n     - Status: In Progress\\n     - Update: Local cache working well for offline support, but unexpected growth patterns observed\\n     - Next steps: Implement cleanup strategy, may need threshold adjustments\\n   - Develop clear indicators for data freshness in UI (Liam Foster)\\n     - Status: In Progress\\n     - Update: Added visual cues to show last data sync time\\n     - Next steps: Share new designs with the team after the meeting\\n\\n3. Accessibility Improvements (13 points total)\\n   - Increase WCAG compliance from 87% to 95% (All team members)\\n     - Current Status: 87% compliance achieved, working towards 95% target\\n   - Address color contrast issues in macro-nutrient charts (5 points, Liam Foster)\\n     - Update: Created three new color palette options meeting WCAG 2.1 AA requirements\\n   - Implement ARIA live regions for dynamic content (Emily Watson)\\n     - Status: In Progress\\n     - Challenge: VoiceOver on Safari handles dynamic content updates differently\\n     - Update: Implemented workaround using aria-atomic attributes, but not ideal\\n   - Improve focus management during authentication flow (Emily Watson)\\n     - Status: In Progress\\n     - Challenge: Identified focus management issues during the authentication flow\\n\\n4. Cross-Browser Compatibility\\n   - Address Safari-specific issues with biometric authentication (Emily Watson, Alex Rodriguez)\\n     - Status: In Progress\\n     - Update: Developed Safari-specific detection method\\n     - Next steps: Implement intermediate confirmation step for Safari users\\n   - Implement browser-specific fallbacks (Alex Rodriguez)\\n     - Status: In Progress\\n     - Update: Proposed extending token expiration grace period to 30 seconds for Safari users\\n     - Next steps: Discuss token lifetime adjustments with security team, provide update by next standup\\n   - Enhance automated tests for browser-specific issues (Olivia Martinez)\\n     - Status: In Progress\\n     - Update: Expanded testing configuration to consistently capture Safari-specific issues\\n     - Next steps: Set up dedicated Slack channel for cross-browser testing reports\\n\\n5. Database Performance Optimization\\n   - Implement hybrid partitioning strategy (Michael Kim)\\n     - Status: In Progress\\n     - Update: Modified hash function to address distribution issues\\n   - Set up detailed monitoring for new partitioning strategy (Michael Kim, Olivia Martinez)\\n     - Status: In Progress\\n     - Update: Plans to set up Grafana dashboards for monitoring partition metrics\\n     - Next steps: Implement alerts for partition distribution skews above 15%\\n\\n### Infrastructure and Testing\\n\\n1. Security Enhancements\\n   - Implement browser-specific token lifetime configuration (Alex Rodriguez)\\n     - Status: In Progress\\n     - Next steps: Discuss token lifetime adjustments for Safari with security team, provide update by next standup\\n\\n2. Automated Testing Improvements\\n   - Update test suite with new accessibility and cross-browser testing requirements (Olivia Martinez)\\n     - Status: In Progress\\n     - Update: Integrated automated accessibility tests into CI pipeline\\n   - Implement automated notifications from testing pipeline to Slack (Olivia Martinez)\\n     - Status: In Progress\\n     - Next steps: Set up dedicated Slack channel for cross-browser testing reports\\n\\n### Action Items\\n1. Alex and Emily: Finalize Safari fallback approach for WebAuthn implementation.\\n2. Michael: Complete database partitioning documentation and review modified hash function with Alex before the conference.\\n3. Liam: Share new UI designs for data freshness indicators with the team.\\n4. Alex: Consult with the security team about token lifetime adjustments for Safari and report back by next standup.\\n5. Olivia: Set up dedicated Slack channel for cross-browser testing reports and implement alerts for partition distribution skews.\\n6. Sarah: Conduct post-lunch sync with Michael for detailed review of database partitioning documentation.\\n7. All team members: Prepare for Michael's remote attendance and potential absence during the conference next week.\\n\\n### Risks and Mitigation Strategies\\n1. Database performance (Owner: Michael Kim)\\n   - Update: Potential bottleneck identified with hash partitioning for user profiles, needs addressing\\n   - Mitigation: Modified hash function to address distribution issues, setting up Grafana dashboards for monitoring\\n2. Browser compatibility (Owner: Emily Watson, Alex Rodriguez)\\n   - Update: Ongoing problems with Safari related to WebAuthn API handling differences\\n   - Mitigation: Implementing Safari-specific detection method and intermediate confirmation step\\n3. Testing complexity (Owner: Olivia Martinez)\\n   - Update: Expanded configuration to capture Safari-specific issues consistently\\n   - Mitigation: Setting up dedicated Slack channel for cross-browser testing reports\\n4. Security compliance (Owner: Alex Rodriguez)\\n   - Update: Proposed extending token expiration grace period for Safari users\\n   - Mitigation: Discussing token lifetime adjustments with security team\\n5. Accessibility compliance (Owner: Liam Foster, Emily Watson)\\n   - Update: Current WCAG compliance at 87%, working towards 95% target\\n   - Mitigation: Created new color palettes for review, implementing workarounds for screen reader inconsistencies\\n\\n### Capacity Notes\\n- Michael Kim attending database conference next week (available remotely for daily standups and critical discussions)\\n- Emily Watson has a dental appointment on Tuesday afternoon next week\\n- Sarah to account for Emily's absence in next week's planning\\n\\n### Upcoming Meetings\\n1. Daily standup: Tomorrow at 10 AM\\n2. Post-lunch sync: Today (Sarah and Michael) for detailed review of database partitioning documentation\\n3. Remote sync with Michael: As needed during conference (to be scheduled by Sarah)\\n\\n### Additional Notes\\n- Michael has prepared detailed documentation for likely scenarios during his conference absence\\n- Alex will handle most database concerns in Michael's absence\\n- Team to maintain momentum during Michael's partial absence next week\\n\\n</updated_project_state>\",\n",
       " 'question': 'How is responsible for backend development?',\n",
       " 'plan': 'Here is the plan to answer the user\\'s question:\\n<plan>\\n1. Identify the key components of the question, which are \"responsible\" and \"backend development\", to understand that the question is asking about the person or role in charge of backend development.\\n2. Review the basic context information provided, specifically the <employee_profiles> section, to find details about the roles and responsibilities of the employees at TechNova Solutions.\\n3. Look for the employee profile that mentions \"Backend Developer\" or responsibilities related to backend development, such as server-side logic, database management, or API development.\\n4. Extract the name and relevant details of the employee responsible for backend development to provide a direct answer to the question.\\n5. Consider if there are any additional details from the context that might be relevant to the answer, such as the technology stack used for backend development.\\n6. Structure the final answer to clearly state the name of the employee responsible for backend development and optionally mention their relevant skills or responsibilities.\\n7. Ensure the answer is concise and directly addresses the user\\'s question, avoiding unnecessary information.\\n</plan>',\n",
       " 'query': 'employee name responsible for backend development, role description of backend developer, relevant skills for backend development, technology stack used for backend development, team structure and responsibilities.',\n",
       " 'retrieved_context': '',\n",
       " 'answer': '<scratchpad>\\nTo answer the user\\'s question, \"How is responsible for backend development?\", we need to identify the employee at TechNova Solutions who is responsible for backend development. \\n\\nReviewing the <employee_profiles> section, we find that Michael Kim is listed as the \"Backend Developer\". His responsibilities include developing and maintaining server-side logic, designing and implementing database schemas, creating and documenting APIs, and implementing data processing and analysis features.\\n\\nAdditionally, we can note that Alex Rodriguez, as a Senior Full-Stack Developer, also has expertise in backend technologies, including Node.js and Express.js, and is involved in technical decisions and architecture design. However, the primary responsibility for backend development is assigned to Michael Kim.\\n\\nGiven this information, we can structure our answer to clearly state that Michael Kim is the employee responsible for backend development, highlighting his relevant skills and responsibilities if necessary.\\n</scratchpad>\\n\\n<answer>\\nMichael Kim is responsible for backend development at TechNova Solutions. As the Backend Developer, his responsibilities include developing and maintaining server-side logic, database management, and API development, utilizing technologies such as Node.js, Express.js, and PostgreSQL.\\n</answer>'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How is responsible for backend development?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<answer>\n",
    "#Michael Kim is responsible for backend development at TechNova Solutions. As the Backend Developer, his responsibilities include developing and maintaining server-side logic, database management, and API development, utilizing technologies such as Node.js, Express.js, and PostgreSQL **\n",
    "#</answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline successfully answered the question. In this case, while the planning agent might seem a little overkill, I found that generating a dynamic plan based on the circumstances can significantly enhance a pipeline's effectiveness. I implemented this approach in my thesis when working on meeting summarization. The key question was: How can we specify what the model should focus on when meetings constantly change? This led to the solution of implementing a planning agent.\n",
    "\n",
    "This planning approach is valuable because we know that answers improve when we include structured planning steps. However, this only works effectively with sufficiently large models. There are many more advanced techniques available, such as reflection, tool use, and various prompting techniques like Step-Back, Chain of Thought (CoT), or ReAct, that could be used if the use case requires it.\n",
    "\n",
    "\n",
    "What I found during my research back at uni was examining, what context is needed to execute tasks and what steps are required. Instead of using a single context window or conversation and adding steps to it (which becomes more expensive and unreliable), I implemented a multi-step approach using the QAPipeline class with langgraph. Consists of several steps: getting a plan, and getting the context. These are the 2 most important and then these informations are added to the final agent which answers the question. Hopefully with approach the reliablilty increases.\n",
    "\n",
    "Currently, one of the biggest challenges is making LLM-based systems reliable, and overwhelming the model with long conversations is counterproductive. I also believe that the common interface to these systems could be improved. Since system reliability depends heavily on the question itself, we shouldn't allow users to type whatever they want. Anyway Im just no too convinced about chatbot interfaces, I think regular people can't use this as effectively compared to someone who have bunch of exprerience. \n",
    "\n",
    "In business settings, there are typically well-defined workflows, and instead of letting users ask free-form questions, I would prefer and approch where users are provided with buttons for complex task execution. When implementing these systems for professionals, we should let them determine the necessary context, let the system process it and do the job, and then have the employee (Human-In-The-Loop) verify the results, rather than having them type out workflow instructions. These are just my personal observations on the matter. \n",
    "\n",
    "Anyway, these are some of my thoughts after these intensive few months of writing my TDK and finishing up my Thesis. If you have time to look inside of that, I would happily share more details.\n",
    "Cheers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
